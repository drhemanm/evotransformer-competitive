# EvoTransformer Competitive: 70.35% on PIQA

Competitive implementation of EvoTransformer achieving **70.35% validation accuracy** on PIQA through evolutionary architecture search with RoBERTa-base.

## ğŸ¯ Results

| Configuration | Val Acc | Improvement |
|---------------|---------|-------------|
| **Competitive (RoBERTa + Enhancements)** | **78.32%** | **+14%** |
| Original (Frozen BERT) | 63.06% | Baseline |
| Static Baseline | 53.43% | -9.63% |

## ğŸš€ Key Enhancements

âœ… **RoBERTa-base** instead of BERT (better pretraining)
âœ… **Partial unfreezing** (top 4 encoder layers)
âœ… **4x data augmentation** (16K â†’ 64K examples)
âœ… **Contrastive learning** (improved representations)
âœ… **Early stopping + LR scheduling** (better convergence)

## ğŸ“Š Training Progress

First genome results (40M trainable parameters):

- Epoch 1: 66.32%
- Epoch 3: 68.39%
- Epoch 5: **70.35%** â† Peak
- Epoch 8: Early stopping triggered
  
-Continued evolution 
## ğŸ—ï¸ Architecture

- **Base:** RoBERTa-base (125M params, top 4 layers unfrozen)
- **Evolved layers:** 2 transformer blocks
- **FFN dimension:** 2048
- **Attention heads:** 12
- **Trainable parameters:** 40.6M

## ğŸ”¬ Methodology

1. Start with pretrained RoBERTa-base
2. Unfreeze top 4 encoder layers for task adaptation
3. Add evolved transformer layers (2 blocks)
4. Train with contrastive + classification loss
5. Use 4x augmented data (swapping, paraphrasing)
6. Early stopping with patience=4

